{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c0733a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10fc654ade24622b90f465a013e746e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorisations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da5cd208ed742d5b5158925a3af1517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mod√®les (tfidf):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a5d211c4f048158f8ba603c797c4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mod√®les (count):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä R√©sultats des mod√®les classiques :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.8752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.8550</td>\n",
       "      <td>0.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.8676</td>\n",
       "      <td>0.8676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.8756</td>\n",
       "      <td>0.8756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>count</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>0.8755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>count</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.8441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>count</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.8708</td>\n",
       "      <td>0.8708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>count</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>0.8754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vectorizer               Model  F1 (macro)  Accuracy\n",
       "0      tfidf  LogisticRegression      0.8752    0.8752\n",
       "1      tfidf        RandomForest      0.8550    0.8550\n",
       "2      tfidf          NaiveBayes      0.8676    0.8676\n",
       "3      tfidf                 SVM      0.8756    0.8756\n",
       "4      count  LogisticRegression      0.8755    0.8755\n",
       "5      count        RandomForest      0.8440    0.8441\n",
       "6      count          NaiveBayes      0.8708    0.8708\n",
       "7      count                 SVM      0.8754    0.8754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. üì• Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Pour BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 2. üìÑ Chargement du dataset\n",
    "df = pd.read_csv(\"data/clean_dataset.csv\")\n",
    "df = df.dropna(subset=['text_processed'])\n",
    "df = df[df['text_processed'].str.strip().astype(bool)]\n",
    "\n",
    "X = df['text_processed']\n",
    "y = df['humor']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. üî† Vectoriseurs et Mod√®les\n",
    "vectorizers = {\n",
    "    \"tfidf\": TfidfVectorizer(max_features=5000),\n",
    "    \"count\": CountVectorizer(max_features=5000)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# 4. üß™ Comparaison avec barre de progression\n",
    "results = []\n",
    "for vec_name, vectorizer in tqdm(vectorizers.items(), desc=\"Vectorisations\"):\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    for model_name, model in tqdm(models.items(), desc=f\"Mod√®les ({vec_name})\", leave=False):\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        results.append({\n",
    "            \"Vectorizer\": vec_name,\n",
    "            \"Model\": model_name,\n",
    "            \"F1 (macro)\": round(report['macro avg']['f1-score'], 4),\n",
    "            \"Accuracy\": round(report['accuracy'], 4)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"üìä R√©sultats des mod√®les classiques :\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e067190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device utilis√© : cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5159089559274a83acfb54bf8d32fd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üîÅ Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764fde30218f407b936f77f0eedd991d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üß™ √âvaluation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Rapport de classification ‚Äî BERT (√©chantillon 5000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85     19999\n",
      "           1       0.83      0.91      0.87     20000\n",
      "\n",
      "    accuracy                           0.86     39999\n",
      "   macro avg       0.86      0.86      0.86     39999\n",
      "weighted avg       0.86      0.86      0.86     39999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üì• Imports (d√©j√† faits, donc √† ignorer si tu les as d√©j√† en haut)\n",
    "\n",
    "# ‚öôÔ∏è Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Device utilis√© :\", device)\n",
    "\n",
    "# üî° Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# üß™ Prendre un √©chantillon de 5000 exemples pour un entra√Ænement rapide\n",
    "sample_df = pd.concat([X_train, y_train], axis=1).sample(5000, random_state=42)\n",
    "X_sample = sample_df['text_processed']\n",
    "y_sample = sample_df['humor']\n",
    "\n",
    "# üìÇ Dataset\n",
    "class HumorDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# üß™ Cr√©ation des datasets et loaders\n",
    "train_dataset = HumorDataset(X_sample, y_sample, tokenizer)\n",
    "test_dataset = HumorDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# üß† Mod√®le BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# ‚öôÔ∏è Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# üîÅ Entra√Ænement\n",
    "model.train()\n",
    "epochs = 1  # reste √† 1 pour rester dans la limite de temps\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"üîÅ Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# üìà √âvaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"üß™ √âvaluation\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "print(\"üìä Rapport de classification ‚Äî BERT (√©chantillon 5000)\")\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ R√©installation propre\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q transformers datasets scikit-learn pandas seaborn matplotlib\n",
    "\n",
    "# üßπ Red√©marre cette cellule juste apr√®s avoir red√©marr√© le runtime !\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, get_scheduler\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üìÇ Chargement des donn√©es\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df['label'] = df['humor'].astype(int)\n",
    "df_sampled = df.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df, val_df, test_df = np.split(df_sampled.sample(frac=1, random_state=42), [int(.8*len(df_sampled)), int(.9*len(df_sampled))])\n",
    "train_df = train_df.rename(columns={'label': 'labels'})\n",
    "test_df = test_df.rename(columns={'label': 'labels'})\n",
    "\n",
    "# üßº Tokenisation\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize(example): return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "train_ds = HFDataset.from_pandas(train_df[['text', 'labels']]).map(tokenize, batched=True)\n",
    "test_ds = HFDataset.from_pandas(test_df[['text', 'labels']]).map(tokenize, batched=True)\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ‚öôÔ∏è Mod√®le\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# üöÇ Entra√Ænement\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# üß™ √âvaluation\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "model.eval(); preds, labels = [], []\n",
    "for batch in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad(): out = model(**batch).logits\n",
    "    preds.extend(torch.argmax(out, dim=-1).cpu().numpy())\n",
    "    labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(f\"\\n‚úÖ F1 score on test set: {f1_score(labels, preds):.4f}\")\n",
    "\n",
    "# üìä Matrice de confusion\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-humor\", \"Humor\"], yticklabels=[\"Non-humor\", \"Humor\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# üìà ROC\n",
    "fpr, tpr, _ = roc_curve(labels, preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc(fpr, tpr):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# üìâ PR Curve\n",
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# üßæ Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "# üîç Erreurs typiques\n",
    "errors = [(i, l, p) for i, (l, p) in enumerate(zip(labels, preds)) if l != p]\n",
    "for idx, true, pred in errors[:5]:\n",
    "    text = test_df.iloc[idx]['text']\n",
    "    print(f\"\\n‚ùå Mal class√© : {text}\")\n",
    "    print(f\"‚úîÔ∏è Vrai label : {true} | üö´ Pr√©dit : {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
