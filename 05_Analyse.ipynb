{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce400e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from datasets import Dataset as HFDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# üìÇ Chargement des donn√©es\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df['label'] = df['humor'].astype(int)\n",
    "df_sampled = df.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "_, _, test_df = np.split(df_sampled.sample(frac=1, random_state=42), [int(.8*len(df_sampled)), int(.9*len(df_sampled))])\n",
    "test_df = test_df.rename(columns={'label': 'labels'})\n",
    "\n",
    "# üßº Tokenisation\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize(example): return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=50)\n",
    "test_ds = HFDataset.from_pandas(test_df[['text', 'labels']]).map(tokenize, batched=True)\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ‚öôÔ∏è Chargement du mod√®le\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# üîç √âvaluation rapide pour afficher les erreurs\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "preds, labels = [], []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"√âvaluation\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch).logits\n",
    "    preds.extend(torch.argmax(out, dim=-1).cpu().numpy())\n",
    "    labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# üîé Affichage des erreurs typiques\n",
    "errors = [(i, l, p) for i, (l, p) in enumerate(zip(labels, preds)) if l != p]\n",
    "for idx, true, pred in errors[:5]:\n",
    "    text = test_df.iloc[idx]['text']\n",
    "    print(f\"\\n‚ùå Mal class√© : {text}\")\n",
    "    print(f\"‚úîÔ∏è Vrai label : {true} | üö´ Pr√©dit : {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ebd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_surprisal(text, model=model, tokenizer=tokenizer):\n",
    "    tokens = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=tokens[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return loss.item()\n",
    "\n",
    "text1 = \"Why don't skeletons fight each other? They don't have the guts.\"\n",
    "text2 = \"The cat sat on the mat.\"\n",
    "\n",
    "print(f\"Surprise 1: {get_surprisal(text1):.4f}\")\n",
    "print(f\"Surprise 2: {get_surprisal(text2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6429049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# English to French\n",
    "src_text = \"Why did the chicken cross the road? To get to the other side!\"\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "tokens = tokenizer(src_text, return_tensors=\"pt\", padding=True)\n",
    "translated = model.generate(**tokens)\n",
    "print(\"FR:\", tokenizer.decode(translated[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
